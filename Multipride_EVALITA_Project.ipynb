{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAlgTK41XF8c"
      },
      "source": [
        "https://www.kaggle.com/datasets/sumitm004/arxiv-scientific-research-papers-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntmvxL-aF24d"
      },
      "outputs": [],
      "source": [
        "!pip install emoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JBaq0uNvz3L"
      },
      "outputs": [],
      "source": [
        "!pip install stanza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6jRZ3DT7j69"
      },
      "outputs": [],
      "source": [
        "!pip install lingua-language-detector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMr8TtJ7-vft"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRPTywc4vA1S"
      },
      "outputs": [],
      "source": [
        "!pip install deep_translator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoshUkzAP-1I"
      },
      "outputs": [],
      "source": [
        "# IMPORT LIBRARIES\n",
        "\n",
        "# Standard Library & System\n",
        "import gc\n",
        "import html\n",
        "import os\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Data Manipulation & Math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# NLP Core & Utilities\n",
        "import emoji\n",
        "import nltk\n",
        "import spacy\n",
        "import stanza\n",
        "from deep_translator import GoogleTranslator\n",
        "from lingua import Language, LanguageDetectorBuilder\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "# Machine Learning (Scikit-learn)\n",
        "import joblib\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, make_scorer\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Deep Learning & Transformers\n",
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "\n",
        "# Environment & Progress Bars\n",
        "from google.colab import drive\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjIU3TH2XFF7"
      },
      "outputs": [],
      "source": [
        "# Mount drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHNhRbnlv7Je"
      },
      "outputs": [],
      "source": [
        "stanza.download('it')\n",
        "stanza.download('en')\n",
        "stanza.download('es')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GygrLdfgEiRE"
      },
      "source": [
        "# DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeHaxU0IDI47"
      },
      "source": [
        "UPLOAD THE DATASETS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baWVFkH4Q7Wv"
      },
      "outputs": [],
      "source": [
        "# Directory\n",
        "DATA_DIR = \"/content/drive/MyDrive/TextMining/Project/Datasets\"\n",
        "\n",
        "# Dataset upload\n",
        "df_it = pd.read_csv(f\"{DATA_DIR}/train_it.csv\")\n",
        "df_es = pd.read_csv(f\"{DATA_DIR}/train_es.csv\")\n",
        "df_en = pd.read_csv(f\"{DATA_DIR}/train_en.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrYaMcAaSKVM"
      },
      "outputs": [],
      "source": [
        "df_it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4wD8RriSNax"
      },
      "outputs": [],
      "source": [
        "df_es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-N9iYaJYSPeO"
      },
      "outputs": [],
      "source": [
        "df_en"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSSzRHP-DMIK"
      },
      "source": [
        "MERGE THE DATASETS INTO A SINGLE ONE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FJw4eYYDHhm"
      },
      "outputs": [],
      "source": [
        "# Full dataset\n",
        "df_all = pd.concat([df_it, df_es, df_en], axis=0, ignore_index=True)\n",
        "\n",
        "df_all.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wqn8SWDCEl6o"
      },
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTAY9MWlOwm0"
      },
      "source": [
        "MISSING VALUES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpBXWMtoEn5Y"
      },
      "outputs": [],
      "source": [
        "# Missing values\n",
        "na_counts = df_all.isna().sum()\n",
        "na_counts.name = \"NA\"\n",
        "na_counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCMrc0DtOyzo"
      },
      "source": [
        "TEXTS AND BIOS THAT ARE NOT CODED IN A STRING FORMAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEo2GZjEIw8l"
      },
      "outputs": [],
      "source": [
        "# Number of observations with a text that's not a string value\n",
        "print(df_all[\"text\"].apply(lambda x: not isinstance(x, str)).sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbVHfgqk5D-0"
      },
      "outputs": [],
      "source": [
        "# Number of observations with a bio that's not a string value\n",
        "print(df_all[\"bio\"].fillna(\"\").apply(lambda x: not isinstance(x, str)).sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re2yN_3tO8j6"
      },
      "source": [
        "DUPLICATED TEXTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUKZR_9SNH5c"
      },
      "outputs": [],
      "source": [
        "# Duplicate texts\n",
        "df_all[df_all[\"text\"].duplicated(keep=False)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsTppaY0AEzE"
      },
      "source": [
        "*5 rows to be removed*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1x_R_oxMNxC1"
      },
      "outputs": [],
      "source": [
        "# Removing duplicated texts\n",
        "df_all = df_all.drop_duplicates(subset=\"text\", keep=\"first\")\n",
        "\n",
        "df_all.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2Nvdft-Zt6Y"
      },
      "source": [
        "CLASS DISTRIBUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qy-jfSQxZsti"
      },
      "outputs": [],
      "source": [
        "# Barplot showing the class distribution\n",
        "dist = df_all[\"label\"].value_counts(normalize=True).plot.bar()\n",
        "\n",
        "# Plot\n",
        "dist.set_title(\"Labels distribution\")\n",
        "dist.set_xlabel(\"Label\")\n",
        "dist.set_ylabel(\"Proportion\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOXYNmavAc3U"
      },
      "source": [
        "*Strong class imbalance*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "md4MZdL0eqs6"
      },
      "outputs": [],
      "source": [
        "# Barplot showing the class distribution by language\n",
        "dist = df_all.groupby(\"lang\")[\"label\"].value_counts(normalize=True).unstack().plot.bar()\n",
        "\n",
        "# Plot\n",
        "dist.set_title(\"Labels distribution by language\")\n",
        "dist.set_xlabel(\"Language\")\n",
        "dist.set_ylabel(\"Proportion\")\n",
        "plt.xticks(rotation=0)\n",
        "dist.legend(title=\"Label\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZdtV-nRiRwW"
      },
      "source": [
        "## TEXT CLEANING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byzuA_fZ_suI"
      },
      "outputs": [],
      "source": [
        "# Function that normalizes all whitespace characters (tabs, newlines) to a single space\n",
        "def clean_whitespaces(text):\n",
        "    if not isinstance(text, str):\n",
        "        return text\n",
        "\n",
        "    return re.sub(r'\\s+', ' ', text).strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4FHsBoLiaB3"
      },
      "outputs": [],
      "source": [
        "# Function that converts emojis into a string format\n",
        "def demojize_emojis(text):\n",
        "    if not isinstance(text, str):\n",
        "        return text\n",
        "\n",
        "    return emoji.demojize(text, delimiters=(\" __\", \"__ \"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewi9ha2Zq_qz"
      },
      "outputs": [],
      "source": [
        "# \"text\" variable cleaning\n",
        "df_all[\"text_processed\"] = (df_all[\"text\"].apply(clean_whitespaces)\n",
        "                                              .apply(demojize_emojis))\n",
        "\n",
        "# \"bio\" variable cleaning\n",
        "df_all[\"bio_processed\"] = (df_all[\"bio\"].apply(clean_whitespaces)\n",
        "                                            .apply(demojize_emojis))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCGciGlpRRa6"
      },
      "source": [
        "## TEXTS AND BIOS ANALYSIS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BZLkxsBO_iH"
      },
      "source": [
        "TEXTS LENGTH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOAvadwDBi0W"
      },
      "outputs": [],
      "source": [
        "# Initialization of the tokenizer for informal texts (mantain emojis, hashtags, mentions, abbreviations, contractions, punctuation, urls)\n",
        "tknzr = TweetTokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dS5AVAo_hdKw"
      },
      "outputs": [],
      "source": [
        "# Extract the tokens from the texts\n",
        "df_all[\"text_tokens\"] = df_all[\"text_processed\"].apply(lambda x: tknzr.tokenize(x))\n",
        "\n",
        "# Count the found tokens in each text\n",
        "df_all[\"text_n_tokens\"] = df_all[\"text_tokens\"].apply(lambda x: len(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GapkOCc2HG6T"
      },
      "outputs": [],
      "source": [
        "# Histogram of the number of tokens per text\n",
        "dist = df_all[\"text_n_tokens\"].hist(bins=25)\n",
        "\n",
        "# Plot\n",
        "dist.set_title(\"Distribution of token counts in texts\")\n",
        "dist.set_xlabel(\"Number of tokens\")\n",
        "dist.set_ylabel(\"Frequency\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LstTB0HdEG_3"
      },
      "outputs": [],
      "source": [
        "# Boxplots per class\n",
        "boxplots = df_all.boxplot(column=\"text_n_tokens\", by=\"label\")\n",
        "\n",
        "# Plot\n",
        "plt.suptitle(\"Boxplots of token count in texts by class\")\n",
        "boxplots.set_title(\"\")\n",
        "boxplots.set_xlabel(\"Label\")\n",
        "boxplots.set_ylabel(\"Number of tokens\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAWlLeWC9ZaW"
      },
      "source": [
        "BIOS LENGTH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DymiWI3f9cUb"
      },
      "outputs": [],
      "source": [
        "# Extract the tokens from the bios\n",
        "df_all[\"bio_tokens\"] = df_all[\"bio_processed\"].apply(lambda x: tknzr.tokenize(x) if isinstance(x, str) else None)\n",
        "\n",
        "# Count the found tokens in each bio\n",
        "df_all[\"bio_n_tokens\"] = df_all[\"bio_tokens\"].apply(lambda x: len(x) if isinstance(x, list) else None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmNtkuS39qlI"
      },
      "outputs": [],
      "source": [
        "# Histogram\n",
        "dist = df_all[\"bio_n_tokens\"].hist(bins=25)\n",
        "\n",
        "# Plot\n",
        "dist.set_title(\"Distribution of token counts in bios\")\n",
        "dist.set_xlabel(\"Number of tokens\")\n",
        "dist.set_ylabel(\"Frequency\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5NYiv7n_5AU"
      },
      "outputs": [],
      "source": [
        "# Boxplots per class\n",
        "boxplots = df_all.boxplot(column=\"bio_n_tokens\", by=\"label\")\n",
        "\n",
        "# Plot\n",
        "plt.suptitle(\"Boxplots of token count in bios by class\")\n",
        "boxplots.set_title(\"\")\n",
        "boxplots.set_xlabel(\"Label\")\n",
        "boxplots.set_ylabel(\"Number of tokens\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ-GkGyzb9Nd"
      },
      "source": [
        "BIO PRESENCE RATE BY CLASS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Axnaac0Ca05-"
      },
      "outputs": [],
      "source": [
        "# Barplot of rows with bio per class\n",
        "dist = (df_all[df_all['lang']!='en'].assign(has_bio=df_all[\"bio\"].notna())\n",
        "                              .groupby(\"label\")[\"has_bio\"]\n",
        "                              .mean())\n",
        "\n",
        "# Plot\n",
        "dist.plot.bar()\n",
        "plt.xlabel(\"Label\")\n",
        "plt.ylabel(\"Proportion of non-NA bios\")\n",
        "plt.title(\"Bio presence rate by class\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiIVyXECBNqd"
      },
      "source": [
        "TEXTS CAPS LOCK RATIO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ri8xhIUGAiP0"
      },
      "outputs": [],
      "source": [
        "# Texts caps lock ratio\n",
        "df_all[\"text_capslock_ratio\"] = df_all[\"text\"].apply(\n",
        "    lambda text: (len([c for c in text if c.isalpha() and c.isupper()]) /\n",
        "                  len([c for c in text if c.isalpha()])))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def proportion_caps_lock_words(text_tokens):\n",
        "    # all the words\n",
        "    word_tokens = [t for t in text_tokens if re.fullmatch(r\"[A-Za-z]+\", t)]\n",
        "    if not word_tokens:\n",
        "        return 0.0\n",
        "    # all the caps lock words\n",
        "    all_caps_words = [t for t in word_tokens if t.isupper()]\n",
        "\n",
        "    return len(all_caps_words) / len(word_tokens)"
      ],
      "metadata": {
        "id": "GxvWndqaJ0wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all[\"text_capslock_ratio\"] = df_all[\"text_tokens\"].apply(proportion_caps_lock_words)"
      ],
      "metadata": {
        "id": "aLX-ZwsKKJO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwfH1wl6AvSw"
      },
      "outputs": [],
      "source": [
        "# Histogram\n",
        "dist = df_all[\"text_capslock_ratio\"].hist(bins=25)\n",
        "\n",
        "# Plot\n",
        "dist.set_title(\"Distribution of caps lock words ratio in texts\")\n",
        "dist.set_xlabel(\"Caps lock ratio\")\n",
        "dist.set_ylabel(\"Frequency\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_pjNoiOCTFd"
      },
      "outputs": [],
      "source": [
        "# Boxplots per class\n",
        "boxplots = df_all.boxplot(column=\"text_capslock_ratio\", by=\"label\")\n",
        "\n",
        "# Plot\n",
        "plt.suptitle(\"Boxplots of caps lock words ratio in texts by class\")\n",
        "boxplots.set_title(\"\")\n",
        "boxplots.set_xlabel(\"Label\")\n",
        "boxplots.set_ylabel(\"Caps lock ratio\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S91MfOUdCLbl"
      },
      "source": [
        "AGGRESSIVE PUNCTATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwPP7Ra8CQPU"
      },
      "outputs": [],
      "source": [
        "# Function that counts how many times a repetition (>2) of ! or ? is present in a text\n",
        "def count_aggressive_punctuation(text):\n",
        "    pattern = r\"[!?]{3,}\"\n",
        "    matches = re.findall(pattern, text)\n",
        "    return len(matches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klEmZ4TzC1lz"
      },
      "outputs": [],
      "source": [
        "# Counts of aggressive punctuation in texts\n",
        "df_all[\"text_aggressive_punct\"] = df_all[\"text\"].apply(count_aggressive_punctuation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRmbnfzhDPU_"
      },
      "outputs": [],
      "source": [
        "# Histogram\n",
        "dist = df_all[\"text_aggressive_punct\"].hist(bins=25)\n",
        "\n",
        "# Plot\n",
        "dist.set_title(\"Distribution of counts of aggressive punctuation in texts\")\n",
        "dist.set_xlabel(\"Counts of aggressive punctuation\")\n",
        "dist.set_ylabel(\"Frequency\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYXlBvXxqdFi"
      },
      "source": [
        "## HASHTAGS ANALYSIS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWEA4Acgo5rR"
      },
      "source": [
        "EXTRACT THE HASHTAGS FROM THE TEXT TOKENS LIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLO38-1iIM13"
      },
      "outputs": [],
      "source": [
        "# Hashtag pattern using regex\n",
        "hashtag_regex = re.compile(r\"^#\\w+$\")\n",
        "\n",
        "# Present hashtags in the text\n",
        "df_all['text_hashtags'] = df_all['text_tokens'].apply(lambda tokens: list(set([t for t in tokens if hashtag_regex.match(t)])) or None)\n",
        "\n",
        "# Proportion of rows with text containing at least an hashtag per class\n",
        "df_all[\"text_has_hashtag\"] = df_all[\"text_hashtags\"].apply(lambda x: 1 if isinstance(x, list) else 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dom5st1DFoj0"
      },
      "source": [
        "HASHTAG PRESENCE RATE BY CLASS IN \"text\" VARIABLE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTltumBwFzWf"
      },
      "outputs": [],
      "source": [
        "# Barplot\n",
        "dist = df_all.groupby(\"label\")[\"text_has_hashtag\"].mean()\n",
        "\n",
        "# Plot\n",
        "dist.plot.bar()\n",
        "plt.xlabel(\"Label\")\n",
        "plt.ylabel(\"Proportion of texts with at least an hashtag\")\n",
        "plt.title(\"Hashtag presence rate by class\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kGy2sVVJB8a"
      },
      "source": [
        "MOST RECURRENT HASHTAGS PER LANGUAGE BY CLASS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXi7lrjLVCCl"
      },
      "outputs": [],
      "source": [
        "# Function to inspect the presence of each hashtags based on the class\n",
        "def hashtags_analysis(df_input, lang):\n",
        "    df = df_input.copy()\n",
        "\n",
        "    # Filter fd by language\n",
        "    df = df[df[\"lang\"] == lang]\n",
        "\n",
        "    # Number of distinct hashtags in the text\n",
        "    df[\"n_hashtags\"] = df[\"text_hashtags\"].apply(\n",
        "        lambda x: len(x) if isinstance(x, (list, str)) else 0\n",
        "        )\n",
        "\n",
        "    # Total number of rows per class\n",
        "    total_rows_0 = df[df[\"label\"] == 0].shape[0]\n",
        "    total_rows_1 = df[df[\"label\"] == 1].shape[0]\n",
        "\n",
        "    # Initialize counters for hashtag frequency distribution\n",
        "    hashtags_class_0 = Counter()\n",
        "    hashtags_class_1 = Counter()\n",
        "\n",
        "    # Inspect only rows with text containing at least an hashtag\n",
        "    for _, row in df[df[\"n_hashtags\"] > 0].iterrows():\n",
        "        for tag in row[\"text_hashtags\"]:\n",
        "            if row[\"label\"] == 0:\n",
        "                hashtags_class_0[tag] += 1\n",
        "            else:\n",
        "                hashtags_class_1[tag] += 1\n",
        "\n",
        "    # Create aggregated dataframe (absent hashtags: from NaN to 0)\n",
        "    tag_df = pd.DataFrame({\n",
        "        \"count_0\": pd.Series(hashtags_class_0),\n",
        "        \"count_1\": pd.Series(hashtags_class_1)\n",
        "    }).fillna(0)\n",
        "\n",
        "    # Proportions per class of rows with a specific hashtag\n",
        "    tag_df[\"prop_0\"] = tag_df[\"count_0\"] / (total_rows_0 + 1e-4)\n",
        "    tag_df[\"prop_1\"] = tag_df[\"count_1\"] / (total_rows_1 + 1e-4)\n",
        "\n",
        "    # Calculate Log-Odds Ratio for each hashtag\n",
        "    #   Formula: log( count_1 / total_1 ) - log( count_0 / total_0 )\n",
        "    tag_df[\"log_odds\"] = np.log(tag_df[\"prop_1\"] + 1e-4) - np.log(tag_df[\"prop_0\"] + 1e-4)\n",
        "\n",
        "    # Only hashtags that appear 10 times in at least one of the two target categories\n",
        "    tag_df = tag_df[(tag_df[\"prop_0\"] >= 0.05) | (tag_df[\"prop_1\"] >= 0.05)]\n",
        "\n",
        "    # Sort dataframe based on the log-odds\n",
        "    return tag_df.sort_values(by=\"log_odds\", ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxzn_MZvBQ2N"
      },
      "outputs": [],
      "source": [
        "# Italian results\n",
        "hashtags_analysis(df_all, lang=\"it\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOraS_RpBmwz"
      },
      "outputs": [],
      "source": [
        "# Spanish results\n",
        "hashtags_analysis(df_all, lang=\"es\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFR3OYjtBmta"
      },
      "outputs": [],
      "source": [
        "# English results\n",
        "hashtags_analysis(df_all, lang=\"en\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcKKmmMLqiAh"
      },
      "source": [
        "## EMOJIS ANALYSIS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixrE7y_fCoTk"
      },
      "source": [
        "RESTORE THE EMOJIS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wf6P6KZDYF4"
      },
      "source": [
        "*\"text_processed\" and \"bio_processed\" variables*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvlqfDZJDYiK"
      },
      "outputs": [],
      "source": [
        "# Function that restores emojis from their demojized version (input: text)\n",
        "def restore_emojis_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return text\n",
        "\n",
        "    # Callback function called for every match found by re.sub\n",
        "    def replace_match(match):\n",
        "        t = match.group(0)\n",
        "\n",
        "        # Needed to replace the emoji\n",
        "        shortcode = t.replace(\"__\", \":\")\n",
        "\n",
        "        # Emoji transformation\n",
        "        em = emoji.emojize(shortcode, language='alias')\n",
        "\n",
        "        # If emoji.emojize fails(it doesn't find the code of the emoji), return the original string\n",
        "        if em != shortcode:\n",
        "            return em\n",
        "        else:\n",
        "            return t\n",
        "\n",
        "    # Replace every occurrence found using the callback logic\n",
        "    return re.sub(r\"__\\S+__\", replace_match, text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYpoPr2dD6vv"
      },
      "outputs": [],
      "source": [
        "# Restore the emojis\n",
        "df_all[\"text_processed\"] = df_all[\"text_processed\"].apply(restore_emojis_text)\n",
        "df_all[\"bio_processed\"] = df_all[\"bio_processed\"].apply(restore_emojis_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fU3UtwW-DW_k"
      },
      "source": [
        "*\"text_tokens\" and \"bio_tokens\" variables*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8v_xqPCgCnx3"
      },
      "outputs": [],
      "source": [
        "# Function that restores the emojis from their demojized version (input: list of tokens)\n",
        "def restore_emojis_list(tokens):\n",
        "    if not isinstance(tokens, list):\n",
        "        return tokens\n",
        "\n",
        "    restored = []\n",
        "\n",
        "    for t in tokens:\n",
        "        # Check if the token is a demojized emoji\n",
        "        if re.compile(r\"^__\\S+__$\").match(t):\n",
        "            # Needed to replace the emoji\n",
        "            shortcode = t.replace(\"__\", \":\")\n",
        "\n",
        "            # Emoji transformation\n",
        "            em = emoji.emojize(shortcode, language='alias')\n",
        "\n",
        "            # If emoji.emojize fails (it doesn't find the code of the emoji), return the original string\n",
        "            if em != shortcode:\n",
        "                restored.append(em)\n",
        "            else:\n",
        "                restored.append(t)\n",
        "        else:\n",
        "            restored.append(t)\n",
        "    return restored"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WNpgvWRC1Wh"
      },
      "outputs": [],
      "source": [
        "# Restore the emojis\n",
        "df_all[\"text_tokens\"] = df_all[\"text_tokens\"].apply(restore_emojis_list)\n",
        "df_all[\"bio_tokens\"] = df_all[\"bio_tokens\"].apply(restore_emojis_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqwrXP_7omUf"
      },
      "source": [
        "EXTRACT THE EMOJIS FROM THE BIO TOKENS LIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhStXDjTJ9Jd"
      },
      "outputs": [],
      "source": [
        "# Present emojis in the bio\n",
        "df_all['bio_emojis'] = df_all['bio_tokens'].apply(lambda tokens: list(set([token for token in tokens if token in emoji.EMOJI_DATA])) or None if isinstance(tokens, list) else None)\n",
        "\n",
        "# Proportion of rows with bio containing at least an emoji per class\n",
        "df_all[\"bio_has_emojis\"] = df_all[\"bio_emojis\"].apply(lambda x: 1 if isinstance(x, list) else 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-2-zpY4PGTn"
      },
      "source": [
        "EMOJIS PRESENCE RATE BY CLASS IN \"bio\" VARIABLE  \n",
        "*Only for observations with bio*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ecaPfK6Mkj9"
      },
      "outputs": [],
      "source": [
        "# Barplot\n",
        "dist = df_all[df_all[\"bio\"].notna()].groupby(\"label\")[\"bio_has_emojis\"].mean()\n",
        "\n",
        "# Plot\n",
        "dist.plot.bar()\n",
        "plt.xlabel(\"Label\")\n",
        "plt.ylabel(\"Proportion of bios with at least an emoji\")\n",
        "plt.title(\"Emojis presence rate by class\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ESZcjYvw3K3"
      },
      "source": [
        "MOST RECURRENT EMOJIS PER CLASS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XktF0qXHzh6o"
      },
      "outputs": [],
      "source": [
        "# Function to inspect the presence of each emoji based on the class\n",
        "def emojis_analysis(df_input):\n",
        "    df = df_input.copy()\n",
        "\n",
        "    # Only rows with a bio\n",
        "    df = df[df[\"bio\"].notna()]\n",
        "\n",
        "    # Number of distinct emojis in the bio\n",
        "    df[\"n_emojis\"] = df[\"bio_emojis\"].fillna(\"\").apply(len)\n",
        "\n",
        "    # Total number of rows per class (NA values excluded)\n",
        "    total_rows_0 = df[df[\"label\"] == 0][\"bio\"].notna().sum()\n",
        "    total_rows_1 = df[df[\"label\"] == 1][\"bio\"].notna().sum()\n",
        "\n",
        "    # Initialize counters for emoji frequency distribution\n",
        "    emojis_class_0 = Counter()\n",
        "    emojis_class_1 = Counter()\n",
        "\n",
        "    # Inspect only rows with bio containing at least an emoji\n",
        "    for _, row in df[df[\"n_emojis\"] > 0].iterrows():\n",
        "        for em in row[\"bio_emojis\"]:\n",
        "            if row[\"label\"] == 0:\n",
        "                emojis_class_0[em] += 1\n",
        "            else:\n",
        "                emojis_class_1[em] += 1\n",
        "\n",
        "    # Create aggregated dataframe (absent emojis: from NaN to 0)\n",
        "    emoji_df = pd.DataFrame({\n",
        "        \"count_0\": pd.Series(emojis_class_0),\n",
        "        \"count_1\": pd.Series(emojis_class_1)\n",
        "    }).fillna(0)\n",
        "\n",
        "    # Proportions per class of rows with a specific emoji\n",
        "    emoji_df[\"prop_0\"] = emoji_df[\"count_0\"] / (total_rows_0 + 1e-4)\n",
        "    emoji_df[\"prop_1\"] = emoji_df[\"count_1\"] / (total_rows_1 + 1e-4)\n",
        "\n",
        "    # Calculate Log-Odds Ratio for each emoji\n",
        "    #   Formula: log( count_1 / total_1 ) - log( count_0 / total_0 )\n",
        "    emoji_df[\"log_odds\"] = np.log(emoji_df[\"prop_1\"] + 1e-4) - np.log(emoji_df[\"prop_0\"] + 1e-4)\n",
        "\n",
        "    # Only emojis that appear 20 times in at least one of the two target categories\n",
        "    emoji_df = emoji_df[(emoji_df[\"count_0\"]>=20) | (emoji_df[\"count_1\"] >= 20)]\n",
        "\n",
        "    # Sort dataframe based on the log-odds\n",
        "    return emoji_df.sort_values(by=\"log_odds\", ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86qNwPlnzpE0"
      },
      "outputs": [],
      "source": [
        "# Results\n",
        "emojis_analysis(df_all)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFLb3f7rQ90u"
      },
      "source": [
        "## PRONOUN PERSON ANALYSIS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlxUMn8oQjZ0"
      },
      "outputs": [],
      "source": [
        "# Function that analyzes a single text and counts the associated person of the words\n",
        "def get_person_counts(text, nlp_model):\n",
        "    # We are interested to know whether a text is in first person or not\n",
        "    c_1st = 0\n",
        "    c_other = 0\n",
        "\n",
        "    # Process text with Stanza\n",
        "    doc = nlp_model(text)\n",
        "\n",
        "    for sent in doc.sentences:\n",
        "        for word in sent.words:\n",
        "            # UPOS (Universal Part of Speech)\n",
        "            if word.upos in [\"PUNCT\", \"SYM\"]:\n",
        "                continue\n",
        "\n",
        "            # Check if the word has morphological features\n",
        "            if not word.feats:\n",
        "                continue\n",
        "\n",
        "            # Determine the person of the current word\n",
        "            is_1st = \"Person=1\" in word.feats\n",
        "            is_other = \"Person=2\" in word.feats or \"Person=3\" in word.feats\n",
        "\n",
        "            if not (is_1st or is_other):\n",
        "                continue\n",
        "\n",
        "            # Cosnider only subjects and verbs\n",
        "            is_subject = (word.deprel == \"nsubj\")\n",
        "            is_verb = (word.upos in [\"VERB\", \"AUX\"])\n",
        "\n",
        "            if not (is_subject or is_verb):\n",
        "                continue\n",
        "\n",
        "            # Syntactic deduplication:\n",
        "            # Check if word is a subject (nsubj) and its head (verb) has the same person\n",
        "            head_index = word.head\n",
        "            # 0 is the root index (represents the main verb of the sentence)\n",
        "            # Stanza indices for the words starts at 1\n",
        "            if head_index > 0:\n",
        "                head_word = sent.words[head_index - 1]\n",
        "\n",
        "                # Dependency relation\n",
        "                if word.deprel == \"nsubj\" and head_word.feats:\n",
        "                    if is_1st and \"Person=1\" in head_word.feats:\n",
        "                        # Skip redundant subject\n",
        "                        continue\n",
        "                    if is_other and (\"Person=2\" in head_word.feats or \"Person=3\" in head_word.feats):\n",
        "                        # Skip redundant subject\n",
        "                        continue\n",
        "\n",
        "            if is_1st:\n",
        "                c_1st += 1\n",
        "            elif is_other:\n",
        "                c_other += 1\n",
        "\n",
        "    return c_1st, c_other"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwXDQzrDQmUx"
      },
      "outputs": [],
      "source": [
        "# Function that processes the dataframe by language to count grammatical person tokens\n",
        "def extract_person_dominance(df):\n",
        "    # Initialize result column\n",
        "    df[\"text_grammar_dominance\"] = \"neutral\"\n",
        "\n",
        "    target_languages = ['it', 'en', 'es']\n",
        "\n",
        "    for lang in target_languages:\n",
        "        print(lang)\n",
        "        # Filter data for the current language\n",
        "        subset_idx = df[df[\"lang\"] == lang].index\n",
        "\n",
        "        # Load Stanza Pipeline\n",
        "        nlp = stanza.Pipeline(lang, processors=\"tokenize,mwt,pos,lemma,depparse\", use_gpu=True, verbose=False)\n",
        "\n",
        "        # Extract texts to process\n",
        "        texts = df.loc[subset_idx, \"text\"].tolist()\n",
        "\n",
        "        # Count the number of times a specific person occurs\n",
        "        results = [get_person_counts(t, nlp) for t in tqdm(texts)]\n",
        "\n",
        "        # Dominance feature\n",
        "        dominance = []\n",
        "        for c_1st, c_other in results:\n",
        "            if c_1st > c_other:\n",
        "                dominance.append(\"1st_person\")\n",
        "            elif c_other > c_1st:\n",
        "                dominance.append(\"other_person\")\n",
        "            else:\n",
        "                dominance.append(\"neutral\")\n",
        "\n",
        "        # Update the dataframe\n",
        "        df.loc[subset_idx, \"text_grammar_dominance\"] = dominance\n",
        "\n",
        "        # Free up memory before loading the next language model\n",
        "        del nlp\n",
        "        gc.collect()\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEI12J_gaJYZ"
      },
      "outputs": [],
      "source": [
        "# Results\n",
        "df_all = extract_person_dominance(df_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaHj3lh2a6Px"
      },
      "outputs": [],
      "source": [
        "# Barplot showing the class distribution by class\n",
        "dist = df_all.groupby(\"label\")[\"text_grammar_dominance\"].value_counts(normalize=True).unstack().plot.bar()\n",
        "\n",
        "# Plot\n",
        "dist.set_title(\"Text dominant pronoun distribution\")\n",
        "dist.set_xlabel(\"Dominant pronoun\")\n",
        "dist.set_ylabel(\"Proportion\")\n",
        "plt.xticks(rotation=0)\n",
        "dist.legend(title=\"Dominant pronoun\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANQ_F1_xyVFr"
      },
      "source": [
        "# FEATURE ENGINEERING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-0m6KmIxxIC"
      },
      "outputs": [],
      "source": [
        "# Dataset cleaning from unnecessary columns\n",
        "df_all = df_all.drop(columns=['text_processed', 'bio_processed', 'text_tokens', 'bio_tokens',\n",
        "                              'text_aggressive_punct', 'text_has_hashtag', 'bio_has_emojis'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzSXEVGRzBgh"
      },
      "outputs": [],
      "source": [
        "# Function to clean the text before extracting features (sentiment, emotion, hatespeech)\n",
        "def text_cleaning(text):\n",
        "    # Decode HTML entities (&amp; -> &)\n",
        "    new_text = html.unescape(text)\n",
        "    # Handle Mentions @user (Cardiff standard)\n",
        "    new_text = re.sub(r'@\\w+', '@user', new_text)\n",
        "    # Handle URLs -> http (Cardiff standard)\n",
        "    new_text = re.sub(r'http\\S+|www\\.\\S+', 'http', new_text)\n",
        "    # Whitespace normalization\n",
        "    new_text = re.sub(r'\\s+', ' ', new_text).strip()\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJBnu1MjQ6ip"
      },
      "source": [
        "## LANGUAGE DETECTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hISuKQR9HkhT"
      },
      "outputs": [],
      "source": [
        "# Detector set-up\n",
        "languages = [Language.ITALIAN, Language.ENGLISH, Language.SPANISH]\n",
        "detector = LanguageDetectorBuilder.from_languages(*languages).build()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4LSIKlnyfBJ"
      },
      "outputs": [],
      "source": [
        "# Function to predict the language of the text\n",
        "def detect_language(text):\n",
        "    # Text cleaning\n",
        "    # URL removal\n",
        "    clean_text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "    # Mention removal\n",
        "    clean_text = re.sub(r'@\\w+', '', clean_text)\n",
        "    # Hashtag removal\n",
        "    clean_text = re.sub(r'#\\w+', '', clean_text)\n",
        "\n",
        "    # Avoid empty texts\n",
        "    if not clean_text:\n",
        "        clean_text = text\n",
        "\n",
        "    # Language detection\n",
        "    detected_language = detector.detect_language_of(clean_text)\n",
        "\n",
        "    if detected_language:\n",
        "        return detected_language.iso_code_639_1.name.lower()\n",
        "\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTTXX1j2dPX_"
      },
      "outputs": [],
      "source": [
        "# Results\n",
        "df_all[\"lang_pred\"] = df_all[\"text\"].apply(detect_language)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrLNgKqnd-R8"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix to display where the model fails\n",
        "confusion_matrix(df_all['lang'], df_all['lang_pred'], labels=['it', 'en', 'es'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6ze9wQ6LczF"
      },
      "outputs": [],
      "source": [
        "# Wrong predictions\n",
        "df_all[df_all[\"lang\"] != df_all[\"lang_pred\"]][[\"text\", \"lang\", \"lang_pred\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hdbgC7exD8V"
      },
      "source": [
        "## TEXT TRANSLATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5a7F7X3bxDQo"
      },
      "outputs": [],
      "source": [
        "# Function that translates spanish and italian texts to english\n",
        "def english_translation(df):\n",
        "    # Translators initialization\n",
        "    trans_it = GoogleTranslator(source=\"it\", target=\"en\")\n",
        "    trans_es = GoogleTranslator(source=\"es\", target=\"en\")\n",
        "\n",
        "    # List to save results into the df\n",
        "    translations = []\n",
        "\n",
        "    # Iterate by rows\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "        # Text info\n",
        "        lang = row[\"lang_pred\"]\n",
        "        text = row[\"text\"]\n",
        "\n",
        "        # If the text is in english it remains the same\n",
        "        if lang == \"it\":\n",
        "            text = trans_it.translate(text)\n",
        "        elif lang == \"es\":\n",
        "            text = trans_es.translate(text)\n",
        "\n",
        "        translations.append(text)\n",
        "\n",
        "    return translations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGcKxBD_xykN"
      },
      "outputs": [],
      "source": [
        "# Results\n",
        "df_all[\"text_translated\"] = english_translation(df_all)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOpUWqgS2eZo"
      },
      "source": [
        "## SENTIMENT ANALYSIS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-ZetZylXqj-"
      },
      "outputs": [],
      "source": [
        "# Upload the model\n",
        "model_sentiment = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "\n",
        "# Defining the classifier pipeline\n",
        "classifier_sentiment = pipeline(\"sentiment-analysis\",\n",
        "                                  model=model_sentiment,\n",
        "                                  tokenizer=model_sentiment,\n",
        "                                  return_all_scores=False,\n",
        "                                  device=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeW7L7jmX43e"
      },
      "outputs": [],
      "source": [
        "# Function to perform sentiment analysis on texts\n",
        "def detect_sentiment(df):\n",
        "    # Texts extraction\n",
        "    texts = df[\"text_translated\"].tolist()\n",
        "    # Text cleaning\n",
        "    texts = [text_cleaning(text) for text in texts]\n",
        "\n",
        "    # Results\n",
        "    results = classifier_sentiment(texts,\n",
        "                                    batch_size=32,\n",
        "                                    truncation=True,\n",
        "                                    max_length=512)\n",
        "\n",
        "    # List to save results into the df\n",
        "    sentiment_labels = []\n",
        "    # Extract the predictions\n",
        "    for res in results:\n",
        "        label = res[\"label\"]\n",
        "        score = res[\"score\"]\n",
        "        # Force the neutral prediction\n",
        "\n",
        "        if score < 0.5:\n",
        "            sentiment_labels.append(\"neutral\")\n",
        "        else:\n",
        "            sentiment_labels.append(label)\n",
        "\n",
        "    return sentiment_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpLtRaDak3Lm"
      },
      "outputs": [],
      "source": [
        "# Extract sentiment\n",
        "df_all[\"text_sentiment\"] = detect_sentiment(df_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LkbxGnZPn8V"
      },
      "outputs": [],
      "source": [
        "# Barplot showing the sentiment distribution by class\n",
        "dist = df_all.groupby(\"label\")[\"text_sentiment\"].value_counts(normalize=True).unstack().plot.bar()\n",
        "\n",
        "# Plot\n",
        "dist.set_title(\"Sentiment distribution by class - Cardiff model\")\n",
        "dist.set_xlabel(\"Label\")\n",
        "dist.set_ylabel(\"Proportion\")\n",
        "plt.xticks(rotation=0)\n",
        "dist.legend(title=\"Sentiment\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGe2uWESLHWq"
      },
      "source": [
        "## EMOTION DETECTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f09WKi5SvhBz"
      },
      "source": [
        "### J-HARTMANN MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1G5JoZdCsAzo"
      },
      "source": [
        "Ekman emotions + neutral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wt7DUXnYM7H_"
      },
      "outputs": [],
      "source": [
        "# Upload the model\n",
        "model_emotion_jhartmann = \"j-hartmann/emotion-english-distilroberta-base\"\n",
        "\n",
        "# Defining the classifier pipeline\n",
        "classifier_emotion_jhartmann = pipeline(\"text-classification\",\n",
        "                                          model=model_emotion_jhartmann,\n",
        "                                          tokenizer=model_emotion_jhartmann,\n",
        "                                          return_all_scores=False,\n",
        "                                          device=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8Z21vMn78Ef"
      },
      "outputs": [],
      "source": [
        "# Function to perform emotion detection\n",
        "def detect_emotion_jhartmann(df):\n",
        "    # Texts extraction\n",
        "    texts = df[\"text_translated\"].tolist()\n",
        "    # Text cleaning\n",
        "    texts = [text_cleaning(text) for text in texts]\n",
        "\n",
        "    # Results\n",
        "    results = classifier_emotion_jhartmann(texts,\n",
        "                                            batch_size=32,\n",
        "                                            truncation=True,\n",
        "                                            max_length=512)\n",
        "\n",
        "    # List to save results into the df\n",
        "    emotion_labels = []\n",
        "    # Extract the predictions\n",
        "    emotion_labels = [res[\"label\"] for res in results]\n",
        "\n",
        "    return emotion_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIP9l_aYNPet"
      },
      "outputs": [],
      "source": [
        "# Extract emotions\n",
        "df_all[\"text_emotion_jhartmann\"] = detect_emotion_jhartmann(df_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bRkN_YrQ0_5"
      },
      "outputs": [],
      "source": [
        "# Barplot showing the emotions distribution by class\n",
        "dist = df_all[df_all[\"text_emotion_jhartmann\"]!=\"neutral\"].groupby(\"label\")[\"text_emotion_jhartmann\"].value_counts(normalize=True).unstack().plot.bar()\n",
        "\n",
        "# Plot\n",
        "dist.set_title(\"Emotion distribution by class - J-Hartmann model\")\n",
        "dist.set_xlabel(\"Labels\")\n",
        "dist.set_ylabel(\"Proportion\")\n",
        "plt.xticks(rotation=0)\n",
        "dist.legend(title=\"Emotions\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yANF8TNesHKA"
      },
      "source": [
        "### SAM-LOWE MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PY5Sz3kQ8XTy"
      },
      "source": [
        "27 GoEmotions emotions + neutral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoiyD4ED8XBU"
      },
      "outputs": [],
      "source": [
        "# Upload the model\n",
        "model_emotion_samlowe = \"SamLowe/roberta-base-go_emotions\"\n",
        "\n",
        "# Defining the classifier pipeline\n",
        "classifier_emotion_samlowe = pipeline(\"text-classification\",\n",
        "                                        model=model_emotion_samlowe,\n",
        "                                        tokenizer=model_emotion_samlowe,\n",
        "                                        return_all_scores=False,\n",
        "                                        device=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FzyVqo18nMw"
      },
      "outputs": [],
      "source": [
        "# Function to perform emotion detection\n",
        "def detect_emotion_samlowe(df):\n",
        "    # Texts extraction\n",
        "    texts = df[\"text_translated\"].tolist()\n",
        "    # Text cleaning\n",
        "    texts = [text_cleaning(text) for text in texts]\n",
        "\n",
        "    # Results\n",
        "    results = classifier_emotion_samlowe(texts,\n",
        "                                          batch_size=32,\n",
        "                                          truncation=True,\n",
        "                                          max_length=512)\n",
        "\n",
        "    # List to save results into the df\n",
        "    emotion_labels = []\n",
        "    # Extract the predictions\n",
        "    emotion_labels = [res[\"label\"] for res in results]\n",
        "\n",
        "    return emotion_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joNaOwr4-JzT"
      },
      "outputs": [],
      "source": [
        "# Extract emotions\n",
        "df_all[\"text_emotion_samlowe\"] = detect_emotion_samlowe(df_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8ePx0rHSVtL"
      },
      "outputs": [],
      "source": [
        "# Barplot showing the emotions distribution by class\n",
        "dist = df_all[df_all[\"text_emotion_samlowe\"]!=\"neutral\"].groupby(\"label\")[\"text_emotion_samlowe\"].value_counts(normalize=True).unstack().plot.bar()\n",
        "\n",
        "# Plot\n",
        "dist.set_title(\"Emotion distribution by class - SamLowe model\")\n",
        "dist.set_xlabel(\"Proportion\")\n",
        "dist.set_ylabel(\"Emotion\")\n",
        "plt.xticks(rotation=0)\n",
        "dist.legend(title=\"Label\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcl17BBzSgGr"
      },
      "source": [
        "## 'SIMPLE' FEATURES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzASPPskiz3c"
      },
      "outputs": [],
      "source": [
        "# Min and max values\n",
        "max_ = df_all[\"text_n_tokens\"].max()\n",
        "min_ = df_all[\"text_n_tokens\"].min()\n",
        "\n",
        "# Normalization\n",
        "df_all[\"text_n_tokens_norm\"] = (df_all[\"text_n_tokens\"] - min_) / (max_ - min_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlnCiY3Hizw3"
      },
      "outputs": [],
      "source": [
        "# Min and max values\n",
        "max_ = df_all[\"bio_n_tokens\"].max()\n",
        "min_ = df_all[\"bio_n_tokens\"].min()\n",
        "\n",
        "# Normalization\n",
        "df_all[\"bio_n_tokens_norm\"] = ((df_all[\"bio_n_tokens\"] - min_) / (max_ - min_)).fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8lYE7gDSkwG"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_all[\"has_rainbow\"] = df_all[\"bio_emojis\"].apply(\n",
        "    lambda x: 1 if isinstance(x, list) and \"\" in x else 0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFdAMfGbSkuM"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_all[\"has_rainbow_flag\"] = df_all[\"bio_emojis\"].apply(\n",
        "    lambda x: 1 if isinstance(x, list) and \"\" in x else 0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvKQ0toUSksY"
      },
      "outputs": [],
      "source": [
        "df_all[\"has_trans_flag\"] = df_all[\"bio_emojis\"].apply(\n",
        "    lambda x: 1 if isinstance(x, list) and \"\" in x else 0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONLPxHpsSkpt"
      },
      "outputs": [],
      "source": [
        "df_all = pd.concat(\n",
        "    [df_all,\n",
        "     pd.get_dummies(df_all[\"text_grammar_dominance\"],\n",
        "                    prefix=\"text_grammar_dominance\",\n",
        "                    drop_first=True).astype(int)\n",
        "    ],\n",
        "    axis=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ahfc9JOZSklh"
      },
      "outputs": [],
      "source": [
        "df_all = pd.concat(\n",
        "    [df_all,\n",
        "     pd.get_dummies(df_all[\"text_sentiment\"],\n",
        "                    prefix=\"text_sentiment\",\n",
        "                    drop_first=True).astype(int)\n",
        "    ],\n",
        "    axis=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_x3Z-UGBSkh_"
      },
      "outputs": [],
      "source": [
        "df_all = pd.concat(\n",
        "    [df_all,\n",
        "     pd.get_dummies(df_all[\"text_emotion_jhartmann\"],\n",
        "                    prefix=\"text_emotion_jhartmann\",\n",
        "                    drop_first=True).astype(int)\n",
        "    ],\n",
        "    axis=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42hPjKalT_Xo"
      },
      "outputs": [],
      "source": [
        "df_all = pd.concat(\n",
        "    [df_all,\n",
        "     pd.get_dummies(df_all[\"text_emotion_samlowe\"],\n",
        "                    prefix=\"text_emotion_samlowe\",\n",
        "                    drop_first=True).astype(int)\n",
        "    ],\n",
        "    axis=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sXdGq3QeiNs"
      },
      "outputs": [],
      "source": [
        "# Save the df in Colab\n",
        "df_all.to_pickle(\"/content/drive/MyDrive/TextMining/Project/Datasets/train_full_processed.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FPeb25gYc8d"
      },
      "source": [
        "# MODELS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OdeekrbusEC"
      },
      "source": [
        "## LOGISTIC REGRESSION WITH TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_all = pd.read_pickle(\"/content/drive/MyDrive/TextMining/Project/Datasets/train_full_processed.pkl\")\n"
      ],
      "metadata": {
        "id": "WVcVmPsHq2sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9gNUqTHTQNn"
      },
      "outputs": [],
      "source": [
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "\n",
        "os.system(\"python -m spacy download it_core_news_sm\")\n",
        "os.system(\"python -m spacy download es_core_news_sm\")\n",
        "\n",
        "# Upload Spacy\n",
        "nlp_en = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
        "nlp_it = spacy.load(\"it_core_news_sm\", disable=['parser', 'ner'])\n",
        "nlp_es = spacy.load(\"es_core_news_sm\", disable=['parser', 'ner'])\n",
        "\n",
        "# Map to choose the model\n",
        "nlp_map = {\"en\": nlp_en, \"it\": nlp_it, \"es\": nlp_es}\n",
        "\n",
        "# Tokenizer initialization\n",
        "tknzr = TweetTokenizer()\n",
        "\n",
        "# Stopwords set\n",
        "stop_en = set(stopwords.words('english'))\n",
        "stop_it = set(stopwords.words('italian'))\n",
        "stop_es = set(stopwords.words('spanish'))\n",
        "stop_words_set = stop_en | stop_it | stop_es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYVVgl1oc2j3"
      },
      "outputs": [],
      "source": [
        "# Function to clean the texts\n",
        "def remove_noise(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # URL removal\n",
        "    text = re.sub(r\"http\\S+|www\\.\\S+\", \"\", text)\n",
        "    # Menzioni removal\n",
        "    text = re.sub(r\"@\\w+\", \"\", text)\n",
        "    # Hashtag removal (only the simbol \"#\")\n",
        "    text = re.sub(r\"#\", \"\", text)\n",
        "    # Mantain only letters\n",
        "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
        "    # Multiple spaces removal\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95uhZO2cUXPm"
      },
      "outputs": [],
      "source": [
        "# Function that split the text into tokens\n",
        "def tokenize(text):\n",
        "    # TweetTokenizer\n",
        "    return tknzr.tokenize(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVvyfH_NUYv5"
      },
      "outputs": [],
      "source": [
        "# Function that removes stopwords\n",
        "def remove_stopwords(tokens):\n",
        "    # Filter common words and too short ones (1 letter)\n",
        "    return [t for t in tokens if t not in stop_words_set and len(t) > 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0SpYk5kUaUL"
      },
      "outputs": [],
      "source": [
        "# Function that reduces words to their lemma\n",
        "def lemmatize(tokens, lang):\n",
        "    nlp_model = nlp_map[lang]\n",
        "    # Spacy works on strings or docs\n",
        "    doc = nlp_model(\" \".join(tokens))\n",
        "    # Lemma extraction\n",
        "    lemmas = [token.lemma_ for token in doc]\n",
        "\n",
        "    return lemmas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcDZ5CTvUcmH"
      },
      "outputs": [],
      "source": [
        "# Function for the whole preprocessing phase\n",
        "def preprocessing_pipeline_tfidf(text, lang):\n",
        "    text_clean = remove_noise(text)\n",
        "    tokens = tokenize(text_clean)\n",
        "    tokens_no_stop = remove_stopwords(tokens)\n",
        "    lemmas = lemmatize(tokens_no_stop, lang)\n",
        "\n",
        "    # TF-IDF want a single string with all the words separated by a space\n",
        "    return \" \".join(lemmas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxywnhtjV4P0"
      },
      "outputs": [],
      "source": [
        "# Texts representation extraction\n",
        "df_all[\"text_tfidf\"] = df_all.apply(\n",
        "    lambda row: (f\"{preprocessing_pipeline_tfidf(row[\"text\"], row[\"lang_pred\"])} {preprocessing_pipeline_tfidf(row[\"bio\"], row[\"lang_pred\"])}\"),\n",
        "    axis=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdoDguUkXiJy"
      },
      "outputs": [],
      "source": [
        "# Dictionary to set the params for the tf-idf configuration\n",
        "tfidf_params = {\n",
        "    \"ngram_range\": (1, 1),      # or (1,2) or (1,3)\n",
        "    \"min_df\": 5,                # ignore rarest words\n",
        "    \"max_df\": 0.9,              # ignore most frequent words\n",
        "    \"max_features\": 5000        # None (considers all the words), 10000 is another potential good choice\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rVZt4cgYXQr"
      },
      "outputs": [],
      "source": [
        "# Vectorizer initialization\n",
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=tfidf_params[\"ngram_range\"],\n",
        "                                    min_df=tfidf_params[\"min_df\"],\n",
        "                                    max_df=tfidf_params[\"max_df\"],\n",
        "                                    max_features=tfidf_params[\"max_features\"],\n",
        "                                    sublinear_tf=True\n",
        "                                    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOjwJDy1ZCAI"
      },
      "outputs": [],
      "source": [
        "# Corpus (set of all texts)\n",
        "corpus = df_all[\"text_tfidf\"].tolist()\n",
        "\n",
        "# Fit (learns vocabulary) + Transform (creates matrix)\n",
        "X = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Word names\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Computes the average of the weights for each word on the whole dataset\n",
        "mean_weights = np.array(X.mean(axis=0)).flatten()\n",
        "\n",
        "# Create a dataframe\n",
        "df_vocab = pd.DataFrame({'word': feature_names, 'average_tfidf': mean_weights})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omYVDZjTZB9G"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_predict, cross_val_score, StratifiedKFold\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Response variable\n",
        "y = df_all['label']\n",
        "\n",
        "# Inizializzazione del modello (resta uguale)\n",
        "clf = LogisticRegression(class_weight='balanced', random_state=44, max_iter=1000)\n",
        "\n",
        "# F1 scores\n",
        "cv_result = cross_val_score(clf, X, y, cv=10, scoring=\"f1_macro\")\n",
        "\n",
        "print(f\"F1-Score (Macro):   {cv_result.mean():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CV (in order to have riproducible splits)\n",
        "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=44)\n",
        "\n",
        "# F1 per class (iteratively treating the current class as the positive one)\n",
        "f1_class0 = cross_val_score(clf, X, y, cv=cv, scoring=make_scorer(f1_score, pos_label=0))\n",
        "f1_class1 = cross_val_score(clf, X, y, cv=cv, scoring=make_scorer(f1_score, pos_label=1))\n",
        "\n",
        "print(f\"F1-Score (classe 0): {f1_class0.mean():.4f}\")\n",
        "print(f\"F1-Score (classe 1): {f1_class1.mean():.4f}\")"
      ],
      "metadata": {
        "id": "G2aS3gwWqZB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYm5y1NTbnUe"
      },
      "source": [
        "## CONTEXTUALIZED EMBEDDINGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zYeOxsW_k-_"
      },
      "outputs": [],
      "source": [
        "# Function that extracts mean-pooled embeddings from text using a pre-trained Transformer model\n",
        "def get_embeddings(df, model_name, variable=\"text\"):\n",
        "    print(f\"--- Extracting Embeddings with: {model_name} ---\")\n",
        "\n",
        "    # Setup device\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Device in use: {device}\")\n",
        "\n",
        "    # Load model and tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModel.from_pretrained(model_name).to(device)\n",
        "    # Evaluation/inference mode (the model is not in a training mode)\n",
        "    model.eval()\n",
        "\n",
        "    # Prepare Texts\n",
        "    texts = df[variable].fillna(\"\").tolist()\n",
        "    all_embeddings = []\n",
        "\n",
        "    # Sequential Processing (One by One)\n",
        "    for text in tqdm(texts):\n",
        "        # Tokenization\n",
        "        encoded_input = tokenizer(text,\n",
        "                                  truncation=True,\n",
        "                                  max_length=256,\n",
        "                                  return_tensors=\"pt\").to(device)\n",
        "\n",
        "        # Inference\n",
        "        with torch.no_grad(): # no gradient computation\n",
        "            model_output = model(**encoded_input)\n",
        "\n",
        "        # Mean pooling:----------------------------------\n",
        "        token_embeddings = model_output.last_hidden_state   # shape (1, L, H)\n",
        "        # Calculate mean and max over the sequence length dimension (dim=1)\n",
        "        sentence_embedding_meanpool = torch.mean(token_embeddings, dim=1).squeeze()\n",
        "        sentence_embedding_maxpool = torch.max(token_embeddings, dim=1).values.squeeze()\n",
        "\n",
        "        # Concatenation of the two poolings\n",
        "        sentence_embedding = torch.cat((sentence_embedding_meanpool, sentence_embedding_maxpool), dim=0)\n",
        "\n",
        "        # Move to CPU (numpy) to save GPU RAM\n",
        "        all_embeddings.append(sentence_embedding.cpu().numpy())\n",
        "\n",
        "    # Concatenate everything into a single giant matrix\n",
        "    final_matrix = np.vstack(all_embeddings)\n",
        "    print(f\"Embeddings extracted! Shape: {final_matrix.shape}\")\n",
        "\n",
        "    return final_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNA_AGU0aZX9"
      },
      "source": [
        "### XLM-RoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ba-6ClbVapWn"
      },
      "outputs": [],
      "source": [
        "# Initialiaze the classifier\n",
        "clf_log_reg = LogisticRegression(class_weight=\"balanced\", max_iter=1000)\n",
        "clf_ridge = RidgeClassifier(class_weight=\"balanced\")\n",
        "clf_svm = SVC(kernel=\"linear\", class_weight=\"balanced\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnOMgTesqIP-"
      },
      "outputs": [],
      "source": [
        "# XLM-RoBERTa\n",
        "model_xlm = \"xlm-roberta-base\"\n",
        "\n",
        "X_train_texts_embeddings = get_embeddings(df_all, model_xlm)\n",
        "X_train_bio_embeddings = get_embeddings(df_all, model_xlm, variable=\"bio\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkLwPkQvJw8R"
      },
      "source": [
        "#### FEATURE ENGINEERING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bOQkIGfZvaE"
      },
      "outputs": [],
      "source": [
        "# Selection of the features from the dataframe to add to the embeddings\n",
        "features_to_add = [\"text_n_tokens_norm\",\n",
        "                   \"bio_n_tokens_norm\",\n",
        "                   \"text_capslock_ratio\",\n",
        "                   \"has_rainbow\",\n",
        "                   \"has_rainbow_flag\",\n",
        "                   \"has_trans_flag\",\n",
        "                   \"text_grammar_dominance_neutral\",\n",
        "                   \"text_grammar_dominance_other_person\",\n",
        "                   \"text_grammar_dominance_neutral\",\n",
        "                   \"text_grammar_dominance_other_person\",\n",
        "                   \"text_sentiment_neutral\",\n",
        "                   \"text_sentiment_positive\",\n",
        "                   \"text_emotion_jhartmann_disgust\",\n",
        "                   \"text_emotion_jhartmann_fear\",\n",
        "                   \"text_emotion_jhartmann_joy\",\n",
        "                   \"text_emotion_jhartmann_neutral\",\n",
        "                   \"text_emotion_jhartmann_sadness\",\n",
        "                   \"text_emotion_jhartmann_surprise\",\n",
        "                   \"text_emotion_samlowe_amusement\",\n",
        "                   \"text_emotion_samlowe_anger\",\n",
        "                   \"text_emotion_samlowe_annoyance\",\n",
        "                   \"text_emotion_samlowe_approval\",\n",
        "                   \"text_emotion_samlowe_caring\",\n",
        "                   \"text_emotion_samlowe_confusion\",\n",
        "                   \"text_emotion_samlowe_curiosity\",\n",
        "                   \"text_emotion_samlowe_desire\",\n",
        "                   \"text_emotion_samlowe_disappointment\",\n",
        "                   \"text_emotion_samlowe_disapproval\",\n",
        "                   \"text_emotion_samlowe_disgust\",\n",
        "                   \"text_emotion_samlowe_embarrassment\",\n",
        "                   \"text_emotion_samlowe_excitement\",\n",
        "                   \"text_emotion_samlowe_fear\",\n",
        "                   \"text_emotion_samlowe_gratitude\",\n",
        "                   \"text_emotion_samlowe_joy\",\n",
        "                   \"text_emotion_samlowe_love\",\n",
        "                   \"text_emotion_samlowe_nervousness\",\n",
        "                   \"text_emotion_samlowe_neutral\",\n",
        "                   \"text_emotion_samlowe_optimism\",\n",
        "                   \"text_emotion_samlowe_pride\",\n",
        "                   \"text_emotion_samlowe_realization\",\n",
        "                   \"text_emotion_samlowe_remorse\",\n",
        "                   \"text_emotion_samlowe_sadness\",\n",
        "                   \"text_emotion_samlowe_surprise\"\n",
        " ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yc1iatjPGpVH"
      },
      "outputs": [],
      "source": [
        "# Extract the numpy matrix to store the adding features\n",
        "X_train_additional_features = df_all[features_to_add].values\n",
        "\n",
        "# Horizontal stack\n",
        "X_train_final = np.hstack((X_train_texts_embeddings, X_train_bio_embeddings, X_train_additional_features))\n",
        "\n",
        "# Target variable\n",
        "y_train = df_all[\"label\"]\n",
        "\n",
        "print(f\"Shape Texts Embeddings: {X_train_texts_embeddings.shape}\")\n",
        "print(f\"Shape Bio Embeddings:   {X_train_bio_embeddings.shape}\")\n",
        "print(f\"Shape Extra Feature:    {X_train_additional_features.shape}\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"FINAL SHAPE (X):       {X_train_final.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MDSonctXHr4"
      },
      "outputs": [],
      "source": [
        "# F1 scores\n",
        "cv_result = cross_val_score(clf_log_reg, X_train_final, y, cv=10, scoring=\"f1_macro\")\n",
        "\n",
        "print(f\"F1-Score (Macro):   {cv_result.mean():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CV (in order to have riproducible splits)\n",
        "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=44)\n",
        "\n",
        "# F1 per class (iteratively treating the current class as the positive one)\n",
        "f1_class0 = cross_val_score(clf_log_reg, X_train_final, y, cv=cv, scoring=make_scorer(f1_score, pos_label=0))\n",
        "f1_class1 = cross_val_score(clf_log_reg, X_train_final, y, cv=cv, scoring=make_scorer(f1_score, pos_label=1))\n",
        "\n",
        "print(f\"F1-Score (classe 0): {f1_class0.mean():.4f}\")\n",
        "print(f\"F1-Score (classe 1): {f1_class1.mean():.4f}\")"
      ],
      "metadata": {
        "id": "lHhe_p-3tKix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QrAzB42nic9"
      },
      "outputs": [],
      "source": [
        "# F1 scores\n",
        "cv_result = cross_val_score(clf_svm, X_train_final, y, cv=10, scoring=\"f1_macro\")\n",
        "\n",
        "print(f\"F1-Score (Macro):   {cv_result.mean():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CV (in order to have riproducible splits)\n",
        "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=44)\n",
        "\n",
        "# F1 per class (iteratively treating the current class as the positive one)\n",
        "f1_class0 = cross_val_score(clf_svm, X_train_final, y, cv=cv, scoring=make_scorer(f1_score, pos_label=0))\n",
        "f1_class1 = cross_val_score(clf_svm, X_train_final, y, cv=cv, scoring=make_scorer(f1_score, pos_label=1))\n",
        "\n",
        "print(f\"F1-Score (classe 0): {f1_class0.mean():.4f}\")\n",
        "print(f\"F1-Score (classe 1): {f1_class1.mean():.4f}\")"
      ],
      "metadata": {
        "id": "mdhHPV6Gtb1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUcKJrMZVJlZ"
      },
      "outputs": [],
      "source": [
        "# F1 scores\n",
        "cv_result = cross_val_score(clf_ridge, X_train_final, y, cv=10, scoring=\"f1_macro\")\n",
        "\n",
        "print(f\"F1-Score (Macro):   {cv_result.mean():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CV (in order to have riproducible splits)\n",
        "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=44)\n",
        "\n",
        "# F1 per class (iteratively treating the current class as the positive one)\n",
        "f1_class0 = cross_val_score(clf_ridge, X_train_final, y, cv=cv, scoring=make_scorer(f1_score, pos_label=0))\n",
        "f1_class1 = cross_val_score(clf_ridge, X_train_final, y, cv=cv, scoring=make_scorer(f1_score, pos_label=1))\n",
        "\n",
        "print(f\"F1-Score (classe 0): {f1_class0.mean():.4f}\")\n",
        "print(f\"F1-Score (classe 1): {f1_class1.mean():.4f}\")"
      ],
      "metadata": {
        "id": "a7MuH7eptf4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTa7MTDyKBFo"
      },
      "source": [
        "#### FEATURE SELECTION"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RFE selector\n",
        "rfe_selector = RFE(clf_log_reg, n_features_to_select=300, step=50)\n",
        "\n",
        "# Fit & Transform\n",
        "X_train_rfe = rfe_selector.fit_transform(X_train_final, y)"
      ],
      "metadata": {
        "id": "8p8e0j02v9yG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# F1 scores\n",
        "cv_result = cross_val_score(clf_log_reg, X_train_rfe, y, cv=10, scoring=\"f1_macro\")\n",
        "\n",
        "print(f\"F1-Score (Macro):   {cv_result.mean():.4f}\")"
      ],
      "metadata": {
        "id": "qX6n2YnswOTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CV (in order to have riproducible splits)\n",
        "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=44)\n",
        "\n",
        "# F1 per class (iteratively treating the current class as the positive one)\n",
        "f1_class0 = cross_val_score(clf_log_reg, X_train_rfe, y, cv=cv, scoring=make_scorer(f1_score, pos_label=0))\n",
        "f1_class1 = cross_val_score(clf_log_reg, X_train_rfe, y, cv=cv, scoring=make_scorer(f1_score, pos_label=1))\n",
        "\n",
        "print(f\"F1-Score (classe 0): {f1_class0.mean():.4f}\")\n",
        "print(f\"F1-Score (classe 1): {f1_class1.mean():.4f}\")"
      ],
      "metadata": {
        "id": "5ZdUrTRSwOQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RFE selector\n",
        "rfe_selector = RFE(clf_svm, n_features_to_select=300, step=50)\n",
        "\n",
        "# Fit & Transform\n",
        "X_train_rfe = rfe_selector.fit_transform(X_train_final, y)"
      ],
      "metadata": {
        "id": "2xMNxRPrwOOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# F1 scores\n",
        "cv_result = cross_val_score(clf_svm, X_train_rfe, y, cv=10, scoring=\"f1_macro\")\n",
        "\n",
        "print(f\"F1-Score (Macro):   {cv_result.mean():.4f}\")"
      ],
      "metadata": {
        "id": "LELCMeeWwPVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CV (in order to have riproducible splits)\n",
        "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=44)\n",
        "\n",
        "# F1 per class (iteratively treating the current class as the positive one)\n",
        "f1_class0 = cross_val_score(clf_svm, X_train_rfe, y, cv=cv, scoring=make_scorer(f1_score, pos_label=0))\n",
        "f1_class1 = cross_val_score(clf_svm, X_train_rfe, y, cv=cv, scoring=make_scorer(f1_score, pos_label=1))\n",
        "\n",
        "print(f\"F1-Score (classe 0): {f1_class0.mean():.4f}\")\n",
        "print(f\"F1-Score (classe 1): {f1_class1.mean():.4f}\")"
      ],
      "metadata": {
        "id": "dOAGeBCXwPSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RFE selector\n",
        "rfe_selector = RFE(clf_ridge, n_features_to_select=300, step=50)\n",
        "\n",
        "# Fit & Transform\n",
        "X_train_rfe = rfe_selector.fit_transform(X_train_final, y)"
      ],
      "metadata": {
        "id": "h4vY8DpowPQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# F1 scores\n",
        "cv_result = cross_val_score(clf_ridge, X_train_rfe, y, cv=10, scoring=\"f1_macro\")\n",
        "\n",
        "print(f\"F1-Score (Macro):   {cv_result.mean():.4f}\")"
      ],
      "metadata": {
        "id": "p85N2UI4wOL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CV (in order to have riproducible splits)\n",
        "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=44)\n",
        "\n",
        "# F1 per class (iteratively treating the current class as the positive one)\n",
        "f1_class0 = cross_val_score(clf_ridge, X_train_rfe, y, cv=cv, scoring=make_scorer(f1_score, pos_label=0))\n",
        "f1_class1 = cross_val_score(clf_ridge, X_train_rfe, y, cv=cv, scoring=make_scorer(f1_score, pos_label=1))\n",
        "\n",
        "print(f\"F1-Score (classe 0): {f1_class0.mean():.4f}\")\n",
        "print(f\"F1-Score (classe 1): {f1_class1.mean():.4f}\")"
      ],
      "metadata": {
        "id": "23Uh7YVrv9vP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM"
      ],
      "metadata": {
        "id": "o6C5AR0V-nod"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8T2bjZkcC0jv"
      },
      "outputs": [],
      "source": [
        "# RFE selector\n",
        "rfe_selector = RFE(clf_svm, n_features_to_select=800, step=10)\n",
        "\n",
        "# Fit & Transform\n",
        "X_train_rfe = rfe_selector.fit_transform(X_train_final, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzYx-e_rti52"
      },
      "outputs": [],
      "source": [
        "# F1 scores\n",
        "cv_result = cross_val_score(clf_svm, X_train_rfe, y, cv=10, scoring=\"f1_macro\")\n",
        "\n",
        "print(f\"F1-Score (Macro):   {cv_result.mean():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CV (in order to have riproducible splits)\n",
        "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=44)\n",
        "\n",
        "# F1 per class (iteratively treating the current class as the positive one)\n",
        "f1_class0 = cross_val_score(clf_svm, X_train_rfe, y, cv=cv, scoring=make_scorer(f1_score, pos_label=0))\n",
        "f1_class1 = cross_val_score(clf_svm, X_train_rfe, y, cv=cv, scoring=make_scorer(f1_score, pos_label=1))\n",
        "\n",
        "print(f\"F1-Score (classe 0): {f1_class0.mean():.4f}\")\n",
        "print(f\"F1-Score (classe 1): {f1_class1.mean():.4f}\")"
      ],
      "metadata": {
        "id": "PFpCc3_xv3gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_-mlmfGtt1x"
      },
      "outputs": [],
      "source": [
        "# Save the selector on drive\n",
        "joblib.dump(rfe_selector,\n",
        "            \"/content/drive/MyDrive/TextMining/Project/Datasets/selector.joblib\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XlkxXFOHeRF"
      },
      "source": [
        "#### HYPER-PARAMETER OPTIMIZATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3o_kqtQ4CYTy"
      },
      "outputs": [],
      "source": [
        "X_train_rfe = rfe_selector.transform(X_train_final)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKcC4rnk6ViH"
      },
      "outputs": [],
      "source": [
        "# Classifier\n",
        "svc = SVC(class_weight='balanced')\n",
        "\n",
        "# Param grid\n",
        "param_grid = {\n",
        "    'kernel': ['linear'],\n",
        "    'C': [0.1, 1, 10, 100]\n",
        "}\n",
        "\n",
        "# Grid search to find the best hyperparameter\n",
        "grid = GridSearchCV(svc, param_grid, cv=10, scoring='f1_macro', n_jobs=-1)\n",
        "grid.fit(X_train_rfe, y)\n",
        "\n",
        "print(grid.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3pETmcE5xmy"
      },
      "outputs": [],
      "source": [
        "# F1 scores\n",
        "cv_result = cross_val_score(clf_svm, X_train_rfe, y_train, cv=10, scoring=\"f1_macro\")\n",
        "\n",
        "print(f\"F1-Score (Macro):   {cv_result.mean():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CV (in order to have riproducible splits)\n",
        "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=44)\n",
        "\n",
        "# F1 per class (iteratively treating the current class as the positive one)\n",
        "f1_class0 = cross_val_score(clf_svm, X_train_rfe, y, cv=cv, scoring=make_scorer(f1_score, pos_label=0))\n",
        "f1_class1 = cross_val_score(clf_svm, X_train_rfe, y, cv=cv, scoring=make_scorer(f1_score, pos_label=1))\n",
        "\n",
        "print(f\"F1-Score (classe 0): {f1_class0.mean():.4f}\")\n",
        "print(f\"F1-Score (classe 1): {f1_class1.mean():.4f}\")"
      ],
      "metadata": {
        "id": "lZL0VxKR28c8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### FINAL TRAINING"
      ],
      "metadata": {
        "id": "8asbUWeo2kEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature selector\n",
        "X_train_rfe = rfe_selector.transform(X_train_final)"
      ],
      "metadata": {
        "id": "YIxyYvdn33gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "clf_svm.fit(X_train_rfe, y_train)"
      ],
      "metadata": {
        "id": "ascnR-NK2o98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the selector on drive\n",
        "joblib.dump(clf_svm,\n",
        "            \"/content/drive/MyDrive/TextMining/Project/Datasets/classifier_svm.joblib\")"
      ],
      "metadata": {
        "id": "DMv4vLQ44uHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyVfjPYfCKDG"
      },
      "source": [
        "# TEST SET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qjiCTqYCNle"
      },
      "outputs": [],
      "source": [
        "# Directory\n",
        "DATA_DIR = \"/content/drive/MyDrive/TextMining/Project/Datasets\"\n",
        "\n",
        "# Dataset upload\n",
        "df_test_it = pd.read_csv(f\"{DATA_DIR}/it_test.csv\")\n",
        "df_test_es = pd.read_csv(f\"{DATA_DIR}/es_test.csv\")\n",
        "df_test_en = pd.read_csv(f\"{DATA_DIR}/en_test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ejmk1rPBCNjd"
      },
      "outputs": [],
      "source": [
        "# Full dataset\n",
        "df_test = pd.concat([df_test_it, df_test_es, df_test_en], axis=0, ignore_index=True)\n",
        "\n",
        "df_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PRE-PROCESSING"
      ],
      "metadata": {
        "id": "UIC8eWoy6I68"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOC0ppBiCNgx"
      },
      "outputs": [],
      "source": [
        "# Function that normalizes all whitespace characters (tabs, newlines) to a single space\n",
        "def clean_whitespaces(text):\n",
        "    if not isinstance(text, str):\n",
        "        return text\n",
        "\n",
        "    return re.sub(r'\\s+', ' ', text).strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbvxoAiHCNeZ"
      },
      "outputs": [],
      "source": [
        "# Function that converts emojis into a string format\n",
        "def demojize_emojis(text):\n",
        "    if not isinstance(text, str):\n",
        "        return text\n",
        "\n",
        "    return emoji.demojize(text, delimiters=(\" __\", \"__ \"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ESpjF4ECNck"
      },
      "outputs": [],
      "source": [
        "# \"text\" variable cleaning\n",
        "df_test[\"text_processed\"] = (df_test[\"text\"].apply(clean_whitespaces)\n",
        "                                              .apply(demojize_emojis))\n",
        "\n",
        "# \"bio\" variable cleaning\n",
        "df_test[\"bio_processed\"] = (df_test[\"bio\"].apply(clean_whitespaces)\n",
        "                                            .apply(demojize_emojis))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyfKNaSOCNaT"
      },
      "outputs": [],
      "source": [
        "# Initialization of the tokenizer for informal texts (mantain emojis, hashtags, mentions, abbreviations, contractions, punctuation, urls)\n",
        "tknzr = TweetTokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmu45Yo8CNXx"
      },
      "outputs": [],
      "source": [
        "# Extract the tokens from the texts\n",
        "df_test[\"text_tokens\"] = df_test[\"text_processed\"].apply(lambda x: tknzr.tokenize(x))\n",
        "\n",
        "# Count the found tokens in each text\n",
        "df_test[\"text_n_tokens\"] = df_test[\"text_tokens\"].apply(lambda x: len(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muOZuFhACNSK"
      },
      "outputs": [],
      "source": [
        "# Extract the tokens from the bios\n",
        "df_test[\"bio_tokens\"] = df_test[\"bio_processed\"].apply(lambda x: tknzr.tokenize(x) if isinstance(x, str) else None)\n",
        "\n",
        "# Count the found tokens in each bio\n",
        "df_test[\"bio_n_tokens\"] = df_test[\"bio_tokens\"].apply(lambda x: len(x) if isinstance(x, list) else None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "so0SBsKBCNQT"
      },
      "outputs": [],
      "source": [
        "# Texts caps lock ratio\n",
        "df_test[\"text_capslock_ratio\"] = df_test[\"text\"].apply(\n",
        "    lambda text: (len([c for c in text if c.isalpha() and c.isupper()]) /\n",
        "                  len([c for c in text if c.isalpha()])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyAcEWCsCNOq"
      },
      "outputs": [],
      "source": [
        "# Function that restores the emojis from their demojized version (input: list of tokens)\n",
        "def restore_emojis_list(tokens):\n",
        "    if not isinstance(tokens, list):\n",
        "        return tokens\n",
        "\n",
        "    restored = []\n",
        "\n",
        "    for t in tokens:\n",
        "        # Check if the token is a demojized emoji\n",
        "        if re.compile(r\"^__\\S+__$\").match(t):\n",
        "            # Needed to replace the emoji\n",
        "            shortcode = t.replace(\"__\", \":\")\n",
        "\n",
        "            # Emoji transformation\n",
        "            em = emoji.emojize(shortcode, language='alias')\n",
        "\n",
        "            # If emoji.emojize fails (it doesn't find the code of the emoji), return the original string\n",
        "            if em != shortcode:\n",
        "                restored.append(em)\n",
        "            else:\n",
        "                restored.append(t)\n",
        "        else:\n",
        "            restored.append(t)\n",
        "    return restored"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTv2pJ1SCNL-"
      },
      "outputs": [],
      "source": [
        "# Restore the emojis\n",
        "df_test[\"text_tokens\"] = df_test[\"text_tokens\"].apply(restore_emojis_list)\n",
        "df_test[\"bio_tokens\"] = df_test[\"bio_tokens\"].apply(restore_emojis_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQDNXi1vCNJ8"
      },
      "outputs": [],
      "source": [
        "# Present emojis in the bio\n",
        "df_test['bio_emojis'] = df_test['bio_tokens'].apply(lambda tokens: list(set([token for token in tokens if token in emoji.EMOJI_DATA])) or None if isinstance(tokens, list) else None)\n",
        "\n",
        "# Proportion of rows with bio containing at least an emoji per class\n",
        "df_test[\"bio_has_emojis\"] = df_test[\"bio_emojis\"].apply(lambda x: 1 if isinstance(x, list) else 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-Fiy-I9CNHm"
      },
      "outputs": [],
      "source": [
        "# Function that analyzes a single text and counts the associated person of the words\n",
        "def get_person_counts(text, nlp_model):\n",
        "    # We are interested to know whether a text is in first person or not\n",
        "    c_1st = 0\n",
        "    c_other = 0\n",
        "\n",
        "    # Process text with Stanza\n",
        "    doc = nlp_model(text)\n",
        "\n",
        "    for sent in doc.sentences:\n",
        "        for word in sent.words:\n",
        "            # UPOS (Universal Part of Speech)\n",
        "            if word.upos in [\"PUNCT\", \"SYM\"]:\n",
        "                continue\n",
        "\n",
        "            # Check if the word has morphological features\n",
        "            if not word.feats:\n",
        "                continue\n",
        "\n",
        "            # Determine the person of the current word\n",
        "            is_1st = \"Person=1\" in word.feats\n",
        "            is_other = \"Person=2\" in word.feats or \"Person=3\" in word.feats\n",
        "\n",
        "            if not (is_1st or is_other):\n",
        "                continue\n",
        "\n",
        "            # Cosnider only subjects and verbs\n",
        "            is_subject = (word.deprel == \"nsubj\")\n",
        "            is_verb = (word.upos in [\"VERB\", \"AUX\"])\n",
        "\n",
        "            if not (is_subject or is_verb):\n",
        "                continue\n",
        "\n",
        "            # Syntactic deduplication:\n",
        "            # Check if word is a subject (nsubj) and its head (verb) has the same person\n",
        "            head_index = word.head\n",
        "            # 0 is the root index (represents the main verb of the sentence)\n",
        "            # Stanza indices for the words starts at 1\n",
        "            if head_index > 0:\n",
        "                head_word = sent.words[head_index - 1]\n",
        "\n",
        "                # Dependency relation\n",
        "                if word.deprel == \"nsubj\" and head_word.feats:\n",
        "                    if is_1st and \"Person=1\" in head_word.feats:\n",
        "                        # Skip redundant subject\n",
        "                        continue\n",
        "                    if is_other and (\"Person=2\" in head_word.feats or \"Person=3\" in head_word.feats):\n",
        "                        # Skip redundant subject\n",
        "                        continue\n",
        "\n",
        "            if is_1st:\n",
        "                c_1st += 1\n",
        "            elif is_other:\n",
        "                c_other += 1\n",
        "\n",
        "    return c_1st, c_other"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUM_wYKJCMO1"
      },
      "outputs": [],
      "source": [
        "# Function that processes the dataframe by language to count grammatical person tokens\n",
        "def extract_person_dominance(df):\n",
        "    # Initialize result column\n",
        "    df[\"text_grammar_dominance\"] = \"neutral\"\n",
        "\n",
        "    target_languages = ['it', 'en', 'es']\n",
        "\n",
        "    for lang in target_languages:\n",
        "        print(lang)\n",
        "        # Filter data for the current language\n",
        "        subset_idx = df[df[\"lang\"] == lang].index\n",
        "\n",
        "        # Load Stanza Pipeline\n",
        "        nlp = stanza.Pipeline(lang, processors=\"tokenize,mwt,pos,lemma,depparse\", use_gpu=True, verbose=False)\n",
        "\n",
        "        # Extract texts to process\n",
        "        texts = df.loc[subset_idx, \"text\"].tolist()\n",
        "\n",
        "        # Count the number of times a specific person occurs\n",
        "        results = [get_person_counts(t, nlp) for t in tqdm(texts)]\n",
        "\n",
        "        # Dominance feature\n",
        "        dominance = []\n",
        "        for c_1st, c_other in results:\n",
        "            if c_1st > c_other:\n",
        "                dominance.append(\"1st_person\")\n",
        "            elif c_other > c_1st:\n",
        "                dominance.append(\"other_person\")\n",
        "            else:\n",
        "                dominance.append(\"neutral\")\n",
        "\n",
        "        # Update the dataframe\n",
        "        df.loc[subset_idx, \"text_grammar_dominance\"] = dominance\n",
        "\n",
        "        # Free up memory before loading the next language model\n",
        "        del nlp\n",
        "        gc.collect()\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzpMCZo_AQ8w"
      },
      "outputs": [],
      "source": [
        "# Results\n",
        "df_test = extract_person_dominance(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reOx7BpzMHxA"
      },
      "outputs": [],
      "source": [
        "# Dataset cleaning from unnecessary columns\n",
        "df_test = df_test.drop(columns=['text_processed', 'bio_processed', 'text_tokens',\n",
        "                                'bio_tokens', 'bio_has_emojis'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoU1arLaMGH5"
      },
      "outputs": [],
      "source": [
        "# Function that translates spanish and italian texts to english\n",
        "def english_translation(df):\n",
        "    # Translators initialization\n",
        "    trans_it = GoogleTranslator(source=\"it\", target=\"en\")\n",
        "    trans_es = GoogleTranslator(source=\"es\", target=\"en\")\n",
        "\n",
        "    # List to save results into the df\n",
        "    translations = []\n",
        "\n",
        "    # Iterate by rows\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "        # Text info\n",
        "        lang = row[\"lang\"]\n",
        "        text = row[\"text\"]\n",
        "\n",
        "        # If the text is in english it remains the same\n",
        "        if lang == \"it\":\n",
        "            text = trans_it.translate(text)\n",
        "        elif lang == \"es\":\n",
        "            text = trans_es.translate(text)\n",
        "\n",
        "        translations.append(text)\n",
        "\n",
        "    return translations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqHCnUtSMGFb"
      },
      "outputs": [],
      "source": [
        "# Results\n",
        "df_test[\"text_translated\"] = english_translation(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHJnAmijMGLM"
      },
      "outputs": [],
      "source": [
        "# Function to clean the text before extracting features (sentiment, emotion, hatespeech)\n",
        "def text_cleaning(text):\n",
        "    # Decode HTML entities (&amp; -> &)\n",
        "    new_text = html.unescape(text)\n",
        "    # Handle Mentions @user (Cardiff standard)\n",
        "    new_text = re.sub(r'@\\w+', '@user', new_text)\n",
        "    # Handle URLs -> http (Cardiff standard)\n",
        "    new_text = re.sub(r'http\\S+|www\\.\\S+', 'http', new_text)\n",
        "    # Whitespace normalization\n",
        "    new_text = re.sub(r'\\s+', ' ', new_text).strip()\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-gULwg8SPAw"
      },
      "outputs": [],
      "source": [
        "# Upload the model\n",
        "model_sentiment = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "\n",
        "# Defining the classifier pipeline\n",
        "classifier_sentiment = pipeline(\"sentiment-analysis\",\n",
        "                                  model=model_sentiment,\n",
        "                                  tokenizer=model_sentiment,\n",
        "                                  return_all_scores=False,\n",
        "                                  device=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vBdro8hSO_G"
      },
      "outputs": [],
      "source": [
        "# Function to perform sentiment analysis on texts\n",
        "def detect_sentiment(df):\n",
        "    # Texts extraction\n",
        "    texts = df[\"text_translated\"].tolist()\n",
        "    # Text cleaning\n",
        "    texts = [text_cleaning(text) for text in texts]\n",
        "\n",
        "    # Results\n",
        "    results = classifier_sentiment(texts,\n",
        "                                    batch_size=32,\n",
        "                                    truncation=True,\n",
        "                                    max_length=512)\n",
        "\n",
        "    # List to save results into the df\n",
        "    sentiment_labels = []\n",
        "    # Extract the predictions\n",
        "    for res in results:\n",
        "        label = res[\"label\"]\n",
        "        score = res[\"score\"]\n",
        "        # Force the neutral prediction\n",
        "\n",
        "        if score < 0.5:\n",
        "            sentiment_labels.append(\"neutral\")\n",
        "        else:\n",
        "            sentiment_labels.append(label)\n",
        "\n",
        "    return sentiment_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHLOuUPTSO85"
      },
      "outputs": [],
      "source": [
        "# Extract sentiment\n",
        "df_test[\"text_sentiment\"] = detect_sentiment(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6Q0PK2oSO51"
      },
      "outputs": [],
      "source": [
        "# Upload the model\n",
        "model_emotion_jhartmann = \"j-hartmann/emotion-english-distilroberta-base\"\n",
        "\n",
        "# Defining the classifier pipeline\n",
        "classifier_emotion_jhartmann = pipeline(\"text-classification\",\n",
        "                                          model=model_emotion_jhartmann,\n",
        "                                          tokenizer=model_emotion_jhartmann,\n",
        "                                          return_all_scores=False,\n",
        "                                          device=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79tR1j1sa2s9"
      },
      "outputs": [],
      "source": [
        "# Function to perform emotion detection\n",
        "def detect_emotion_jhartmann(df):\n",
        "    # Texts extraction\n",
        "    texts = df[\"text_translated\"].tolist()\n",
        "    # Text cleaning\n",
        "    texts = [text_cleaning(text) for text in texts]\n",
        "\n",
        "    # Results\n",
        "    results = classifier_emotion_jhartmann(texts,\n",
        "                                            batch_size=32,\n",
        "                                            truncation=True,\n",
        "                                            max_length=512)\n",
        "\n",
        "    # List to save results into the df\n",
        "    emotion_labels = []\n",
        "    # Extract the predictions\n",
        "    emotion_labels = [res[\"label\"] for res in results]\n",
        "\n",
        "    return emotion_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "om6qd1AAa2pm"
      },
      "outputs": [],
      "source": [
        "# Extract emotions\n",
        "df_test[\"text_emotion_jhartmann\"] = detect_emotion_jhartmann(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cj_GHgmva2nj"
      },
      "outputs": [],
      "source": [
        "# Upload the model\n",
        "model_emotion_samlowe = \"SamLowe/roberta-base-go_emotions\"\n",
        "\n",
        "# Defining the classifier pipeline\n",
        "classifier_emotion_samlowe = pipeline(\"text-classification\",\n",
        "                                        model=model_emotion_samlowe,\n",
        "                                        tokenizer=model_emotion_samlowe,\n",
        "                                        return_all_scores=False,\n",
        "                                        device=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5RlQ8Ona2lo"
      },
      "outputs": [],
      "source": [
        "# Function to perform emotion detection\n",
        "def detect_emotion_samlowe(df):\n",
        "    # Texts extraction\n",
        "    texts = df[\"text_translated\"].tolist()\n",
        "    # Text cleaning\n",
        "    texts = [text_cleaning(text) for text in texts]\n",
        "\n",
        "    # Results\n",
        "    results = classifier_emotion_samlowe(texts,\n",
        "                                          batch_size=32,\n",
        "                                          truncation=True,\n",
        "                                          max_length=512)\n",
        "\n",
        "    # List to save results into the df\n",
        "    emotion_labels = []\n",
        "    # Extract the predictions\n",
        "    emotion_labels = [res[\"label\"] for res in results]\n",
        "\n",
        "    return emotion_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kdqt8jFaa2jh"
      },
      "outputs": [],
      "source": [
        "# Extract emotions\n",
        "df_test[\"text_emotion_samlowe\"] = detect_emotion_samlowe(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Baz4X-Ja2hX"
      },
      "outputs": [],
      "source": [
        "# Min and max values\n",
        "max_ = df_test[\"text_n_tokens\"].max()\n",
        "min_ = df_test[\"text_n_tokens\"].min()\n",
        "\n",
        "# Normalization\n",
        "df_test[\"text_n_tokens_norm\"] = (df_test[\"text_n_tokens\"] - min_) / (max_ - min_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyEo6di4a2fb"
      },
      "outputs": [],
      "source": [
        "# Min and max values\n",
        "max_ = df_test[\"bio_n_tokens\"].max()\n",
        "min_ = df_test[\"bio_n_tokens\"].min()\n",
        "\n",
        "# Normalization\n",
        "df_test[\"bio_n_tokens_norm\"] = ((df_test[\"bio_n_tokens\"] - min_) / (max_ - min_)).fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdmecoLGa2dZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_test[\"has_rainbow\"] = df_test[\"bio_emojis\"].apply(\n",
        "    lambda x: 1 if isinstance(x, list) and \"\" in x else 0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Aozbs-Fa2be"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_test[\"has_rainbow_flag\"] = df_test[\"bio_emojis\"].apply(\n",
        "    lambda x: 1 if isinstance(x, list) and \"\" in x else 0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYWbne3oMGDM"
      },
      "outputs": [],
      "source": [
        "df_test[\"has_trans_flag\"] = df_test[\"bio_emojis\"].apply(\n",
        "    lambda x: 1 if isinstance(x, list) and \"\" in x else 0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ew_ajpe3bTnv"
      },
      "outputs": [],
      "source": [
        "df_test = pd.concat(\n",
        "    [df_test,\n",
        "     pd.get_dummies(df_test[\"text_grammar_dominance\"],\n",
        "                    prefix=\"text_grammar_dominance\",\n",
        "                    drop_first=True).astype(int)\n",
        "    ],\n",
        "    axis=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLwNeN42bTlQ"
      },
      "outputs": [],
      "source": [
        "df_test = pd.concat(\n",
        "    [df_test,\n",
        "     pd.get_dummies(df_test[\"text_sentiment\"],\n",
        "                    prefix=\"text_sentiment\",\n",
        "                    drop_first=True).astype(int)\n",
        "    ],\n",
        "    axis=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_evS1ZQybTi-"
      },
      "outputs": [],
      "source": [
        "df_test = pd.concat(\n",
        "    [df_test,\n",
        "     pd.get_dummies(df_test[\"text_emotion_jhartmann\"],\n",
        "                    prefix=\"text_emotion_jhartmann\",\n",
        "                    drop_first=True).astype(int)\n",
        "    ],\n",
        "    axis=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0N_b4AG6bTgw"
      },
      "outputs": [],
      "source": [
        "df_test = pd.concat(\n",
        "    [df_test,\n",
        "     pd.get_dummies(df_test[\"text_emotion_samlowe\"],\n",
        "                    prefix=\"text_emotion_samlowe\",\n",
        "                    drop_first=True).astype(int)\n",
        "    ],\n",
        "    axis=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PREDICTIONS"
      ],
      "metadata": {
        "id": "XT0UbO3_5-6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the df\n",
        "df_test = pd.read_pickle(\"/content/drive/MyDrive/TextMining/Project/Datasets/test_full_processed.pkl\")"
      ],
      "metadata": {
        "id": "ASw66KDS06_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kt_B8xaZMHt3"
      },
      "outputs": [],
      "source": [
        "# Function that extracts mean-pooled embeddings from text using a pre-trained Transformer model\n",
        "def get_embeddings(df, model_name, variable=\"text\"):\n",
        "    print(f\"--- Extracting Embeddings with: {model_name} ---\")\n",
        "\n",
        "    # Setup device\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Device in use: {device}\")\n",
        "\n",
        "    # Load model and tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModel.from_pretrained(model_name).to(device)\n",
        "    # Evaluation/inference mode (the model is not in a training mode)\n",
        "    model.eval()\n",
        "\n",
        "    # Prepare Texts\n",
        "    texts = df[variable].fillna(\"\").tolist()\n",
        "    all_embeddings = []\n",
        "\n",
        "    # Sequential Processing (One by One)\n",
        "    for text in tqdm(texts):\n",
        "        # Tokenization\n",
        "        encoded_input = tokenizer(text,\n",
        "                                  truncation=True,\n",
        "                                  max_length=256,\n",
        "                                  return_tensors=\"pt\").to(device)\n",
        "\n",
        "        # Inference\n",
        "        with torch.no_grad(): # no gradient computation\n",
        "            model_output = model(**encoded_input)\n",
        "\n",
        "        # Mean pooling:----------------------------------\n",
        "        token_embeddings = model_output.last_hidden_state   # shape (1, L, H)\n",
        "        # Calculate mean and max over the sequence length dimension (dim=1)\n",
        "        sentence_embedding_meanpool = torch.mean(token_embeddings, dim=1).squeeze()\n",
        "        sentence_embedding_maxpool = torch.max(token_embeddings, dim=1).values.squeeze()\n",
        "\n",
        "        # Concatenation of the two poolings\n",
        "        sentence_embedding = torch.cat((sentence_embedding_meanpool, sentence_embedding_maxpool), dim=0)\n",
        "\n",
        "        # Move to CPU (numpy) to save GPU RAM\n",
        "        all_embeddings.append(sentence_embedding.cpu().numpy())\n",
        "\n",
        "    # Concatenate everything into a single giant matrix\n",
        "    final_matrix = np.vstack(all_embeddings)\n",
        "    print(f\"Embeddings extracted! Shape: {final_matrix.shape}\")\n",
        "\n",
        "    return final_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkt3pXCmMHpZ"
      },
      "outputs": [],
      "source": [
        "# XLM-RoBERTa\n",
        "# model_xlm = \"xlm-roberta-base\"\n",
        "\n",
        "# Embeddings\n",
        "X_test_texts_embeddings = get_embeddings(df_test, model_xlm)\n",
        "X_test_bio_embeddings = get_embeddings(df_test, model_xlm, variable=\"bio\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UuYh2OT6MHnZ"
      },
      "outputs": [],
      "source": [
        "# Selection of the features from the dataframe to add to the embeddings\n",
        "features_to_add = [\"text_n_tokens_norm\",\n",
        "                   \"bio_n_tokens_norm\",\n",
        "                   \"text_capslock_ratio\",\n",
        "                   \"has_rainbow\",\n",
        "                   \"has_rainbow_flag\",\n",
        "                   \"has_trans_flag\",\n",
        "                   \"text_grammar_dominance_neutral\",\n",
        "                   \"text_grammar_dominance_other_person\",\n",
        "                   \"text_grammar_dominance_neutral\",\n",
        "                   \"text_grammar_dominance_other_person\",\n",
        "                   \"text_sentiment_neutral\",\n",
        "                   \"text_sentiment_positive\",\n",
        "                   \"text_emotion_jhartmann_disgust\",\n",
        "                   \"text_emotion_jhartmann_fear\",\n",
        "                   \"text_emotion_jhartmann_joy\",\n",
        "                   \"text_emotion_jhartmann_neutral\",\n",
        "                   \"text_emotion_jhartmann_sadness\",\n",
        "                   \"text_emotion_jhartmann_surprise\",\n",
        "                   \"text_emotion_samlowe_amusement\",\n",
        "                   \"text_emotion_samlowe_anger\",\n",
        "                   \"text_emotion_samlowe_annoyance\",\n",
        "                   \"text_emotion_samlowe_approval\",\n",
        "                   \"text_emotion_samlowe_caring\",\n",
        "                   \"text_emotion_samlowe_confusion\",\n",
        "                   \"text_emotion_samlowe_curiosity\",\n",
        "                   \"text_emotion_samlowe_desire\",\n",
        "                   \"text_emotion_samlowe_disappointment\",\n",
        "                   \"text_emotion_samlowe_disapproval\",\n",
        "                   \"text_emotion_samlowe_disgust\",\n",
        "                   \"text_emotion_samlowe_embarrassment\",\n",
        "                   \"text_emotion_samlowe_excitement\",\n",
        "                   \"text_emotion_samlowe_fear\",\n",
        "                   \"text_emotion_samlowe_gratitude\",\n",
        "                   \"text_emotion_samlowe_joy\",\n",
        "                   \"text_emotion_samlowe_love\",\n",
        "                   \"text_emotion_samlowe_nervousness\",\n",
        "                   \"text_emotion_samlowe_neutral\",\n",
        "                   \"text_emotion_samlowe_optimism\",\n",
        "                   \"text_emotion_samlowe_pride\",\n",
        "                   \"text_emotion_samlowe_realization\",\n",
        "                   \"text_emotion_samlowe_remorse\",\n",
        "                   \"text_emotion_samlowe_sadness\",\n",
        "                   \"text_emotion_samlowe_surprise\"\n",
        " ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8FbG4ayAQ1O"
      },
      "outputs": [],
      "source": [
        "# Features to add\n",
        "X_test_additional_features = df_test[features_to_add].values\n",
        "\n",
        "# Horizontal Stack (concatenation)\n",
        "X_test_final = np.hstack((X_test_texts_embeddings,\n",
        "                          X_test_bio_embeddings,\n",
        "                          X_test_additional_features))\n",
        "\n",
        "print(f\"Shape Text Embeddings: {X_test_texts_embeddings.shape}\")\n",
        "print(f\"Shape Bio Embeddings:   {X_test_bio_embeddings.shape}\")\n",
        "print(f\"Shape Extra Feature:    {X_test_additional_features.shape}\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"FINAL SHAPE (X):       {X_test_final.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVfzO2YcAQzG"
      },
      "outputs": [],
      "source": [
        "# Upload the selector\n",
        "rfe_selector = joblib.load('/content/drive/MyDrive/TextMining/Project/Datasets/selector.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RFE transform\n",
        "X_test_rfe = rfe_selector.transform(X_test_final)"
      ],
      "metadata": {
        "id": "qQFI1q0p1eUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the classifier\n",
        "clf_svm = joblib.load('/content/drive/MyDrive/TextMining/Project/Datasets/classifier_svm.joblib')"
      ],
      "metadata": {
        "id": "zb_hZroo5nlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_pred = clf_svm.predict(X_test_rfe)"
      ],
      "metadata": {
        "id": "yD2Jia6A1eRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test_predictions = pd.DataFrame({'id': df_test['id'],\n",
        "                                    'label': y_test_pred\n",
        "                                    'lang'; df_test['lang']\n",
        "                                    })"
      ],
      "metadata": {
        "id": "CsMNGU3qIEfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test_predictions.to_csv('/content/drive/MyDrive/TextMining/Project/df_test_predictions.csv', index=False)"
      ],
      "metadata": {
        "id": "tfdp4larIEdy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "GygrLdfgEiRE",
        "Wqn8SWDCEl6o",
        "UZdtV-nRiRwW",
        "UCGciGlpRRa6",
        "RYXlBvXxqdFi",
        "ZcKKmmMLqiAh",
        "kFLb3f7rQ90u",
        "ANQ_F1_xyVFr",
        "dJBnu1MjQ6ip",
        "8hdbgC7exD8V",
        "MOpUWqgS2eZo",
        "MGe2uWESLHWq",
        "f09WKi5SvhBz",
        "yANF8TNesHKA",
        "jcl17BBzSgGr",
        "6FPeb25gYc8d",
        "3OdeekrbusEC",
        "vYm5y1NTbnUe",
        "UNA_AGU0aZX9",
        "EkLwPkQvJw8R",
        "uTa7MTDyKBFo",
        "3XlkxXFOHeRF",
        "8asbUWeo2kEq",
        "EyVfjPYfCKDG",
        "UIC8eWoy6I68",
        "XT0UbO3_5-6W"
      ],
      "toc_visible": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}